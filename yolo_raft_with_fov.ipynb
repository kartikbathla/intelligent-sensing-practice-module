{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49857aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV width: 33380.05 meters\n",
      "Meters per pixel: 26.078162\n",
      "Processing only first 240 frames (~20 seconds)\n",
      "[Frame 10] Detected 1 raw objects.\n",
      "[Frame 10] Merged into 1 consolidated objects.\n",
      "[Frame 20] Detected 5 raw objects.\n",
      "[Frame 20] Merged into 1 consolidated objects.\n",
      "[Frame 30] Detected 3 raw objects.\n",
      "[Frame 30] Merged into 1 consolidated objects.\n",
      "[Frame 40] Detected 3 raw objects.\n",
      "[Frame 40] Merged into 1 consolidated objects.\n",
      "[Frame 50] Detected 2 raw objects.\n",
      "[Frame 50] Merged into 1 consolidated objects.\n",
      "[Frame 60] Detected 3 raw objects.\n",
      "[Frame 60] Merged into 1 consolidated objects.\n",
      "[Frame 70] Detected 3 raw objects.\n",
      "[Frame 70] Merged into 2 consolidated objects.\n",
      "[Frame 80] Detected 4 raw objects.\n",
      "[Frame 80] Merged into 1 consolidated objects.\n",
      "[Frame 90] Detected 2 raw objects.\n",
      "[Frame 90] Merged into 1 consolidated objects.\n",
      "[Frame 100] Detected 3 raw objects.\n",
      "[Frame 100] Merged into 1 consolidated objects.\n",
      "[Frame 110] Detected 3 raw objects.\n",
      "[Frame 110] Merged into 1 consolidated objects.\n",
      "[Frame 120] Detected 3 raw objects.\n",
      "[Frame 120] Merged into 2 consolidated objects.\n",
      "[Frame 130] Detected 4 raw objects.\n",
      "[Frame 130] Merged into 1 consolidated objects.\n",
      "[Frame 140] Detected 2 raw objects.\n",
      "[Frame 140] Merged into 1 consolidated objects.\n",
      "[Frame 150] Detected 3 raw objects.\n",
      "[Frame 150] Merged into 2 consolidated objects.\n",
      "[Frame 160] Detected 3 raw objects.\n",
      "[Frame 160] Merged into 1 consolidated objects.\n",
      "[Frame 170] Detected 3 raw objects.\n",
      "[Frame 170] Merged into 1 consolidated objects.\n",
      "[Frame 180] Detected 2 raw objects.\n",
      "[Frame 180] Merged into 2 consolidated objects.\n",
      "[Frame 190] Detected 2 raw objects.\n",
      "[Frame 190] Merged into 1 consolidated objects.\n",
      "[Frame 200] Detected 4 raw objects.\n",
      "[Frame 200] Merged into 2 consolidated objects.\n",
      "[Frame 210] Detected 5 raw objects.\n",
      "[Frame 210] Merged into 2 consolidated objects.\n",
      "[Frame 220] Detected 2 raw objects.\n",
      "[Frame 220] Merged into 2 consolidated objects.\n",
      "[Frame 230] Detected 4 raw objects.\n",
      "[Frame 230] Merged into 2 consolidated objects.\n",
      "[Frame 240] Detected 2 raw objects.\n",
      "[Frame 240] Merged into 1 consolidated objects.\n",
      "Reached frame limit (240), stopping early.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Imports --------------------------- #\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "from geopy.distance import geodesic\n",
    "import copy\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.ndimage\n",
    "from ultralytics import YOLO\n",
    "from RAFT.core.raft import RAFT\n",
    "from RAFT.core.utils.utils import InputPadder\n",
    "\n",
    "# --------------------------- Settings --------------------------- #\n",
    "video_path =  #insert path\n",
    "output_path = #insert path\n",
    "yolo_model = #YOLO(\"best.pt\")\n",
    "raft_model_path = #\"RAFT/models/raft-sintel.pth\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "interval = 10  # Process every 10th frame\n",
    "\n",
    "# --------------------------- Load RAFT --------------------------- #\n",
    "args = SimpleNamespace(small=False, mixed_precision=False, alternate_corr=False)\n",
    "raft = RAFT(args)\n",
    "\n",
    "def strip_module_prefix(state_dict):\n",
    "    return {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "raft.load_state_dict(strip_module_prefix(torch.load(raft_model_path, map_location=DEVICE)))\n",
    "raft.to(DEVICE)\n",
    "raft.eval()\n",
    "\n",
    "# --------------------------- Camera Data Helper --------------------------- #\n",
    "def get_camera_json(request_timestamp):\n",
    "    URL = 'https://cameras.alertcalifornia.org/public-camera-data/all_cameras-v3.json?rqts='\n",
    "    unix_timestamp = int(request_timestamp.timestamp())\n",
    "    URL += str(unix_timestamp)\n",
    "    response = requests.get(URL)\n",
    "    return response.json()\n",
    "\n",
    "# --------------------------- Load Video --------------------------- #\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps * 0.25, (w, h))  # Slow motion output\n",
    "\n",
    "# --------------------------- Get Camera FOV Info --------------------------- #\n",
    "now = datetime.now()\n",
    "camera_data = get_camera_json(now)\n",
    "\n",
    "# Assuming you know your camera axis name, e.g., \"axis-eaton\"\n",
    "camera_axis_name = \"Axis-AlabamaHills1\"\n",
    "\n",
    "# Find the camera from the API response\n",
    "camera_info = next((cam for cam in camera_data['features'] if cam['properties']['id'].lower() == camera_axis_name.lower()), None)\n",
    "if camera_info is None:\n",
    "    raise ValueError(f\"Camera {camera_axis_name} not found!\")\n",
    "\n",
    "props = camera_info['properties']\n",
    "\n",
    "# Swap fov_lft and fov_rt from (lon, lat) âž” (lat, lon)\n",
    "left_coord = (props['fov_lft'][1], props['fov_lft'][0])  # (lat, lon)\n",
    "right_coord = (props['fov_rt'][1], props['fov_rt'][0])   # (lat, lon)\n",
    "\n",
    "# Calculate real-world width across the frame\n",
    "fov_width_meters = geodesic(left_coord, right_coord).meters\n",
    "\n",
    "# Meters per pixel\n",
    "meters_per_pixel = fov_width_meters / w\n",
    "\n",
    "print(f\"FOV width: {fov_width_meters:.2f} meters\")\n",
    "print(f\"Meters per pixel: {meters_per_pixel:.6f}\")\n",
    "\n",
    "# --------------------------- Main Processing Loop --------------------------- #\n",
    "ret, prev_frame = cap.read()\n",
    "frame_index = 0\n",
    "# --------------------------- Setup Limit on Frames --------------------------- #\n",
    "video_duration_seconds = 20 # <-- Only process first 10 seconds\n",
    "frame_limit = int(fps * video_duration_seconds)\n",
    "print(f\"Processing only first {frame_limit} frames (~{video_duration_seconds} seconds)\")\n",
    "\n",
    "while ret:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_index += 1\n",
    "\n",
    "    if frame_index > frame_limit:\n",
    "        print(f\"Reached frame limit ({frame_limit}), stopping early.\")\n",
    "        break\n",
    "\n",
    "    if frame_index % interval == 0:\n",
    "        frame2 = frame.copy()\n",
    "        frame1 = prev_frame.copy()\n",
    "\n",
    "        results = yolo_model(frame2, verbose=False)[0]\n",
    "        boxes = [box.xyxy[0].int().tolist() for box in results.boxes if box.conf > 0.05]\n",
    "        print(f\"[Frame {frame_index}] Detected {len(boxes)} raw objects.\")\n",
    "\n",
    "        merged_boxes = merge_boxes(boxes, iou_threshold=0.3)\n",
    "        print(f\"[Frame {frame_index}] Merged into {len(merged_boxes)} consolidated objects.\")\n",
    "\n",
    "        for x1, y1, x2, y2 in merged_boxes:\n",
    "            crop1 = frame1[y1:y2, x1:x2]\n",
    "            crop2 = frame2[y1:y2, x1:x2]\n",
    "\n",
    "            if crop1.shape[0] < 10 or crop1.shape[1] < 10:\n",
    "                continue\n",
    "\n",
    "            image1 = torch.from_numpy(crop1).permute(2, 0, 1).float()[None].to(DEVICE) / 255.0\n",
    "            image2 = torch.from_numpy(crop2).permute(2, 0, 1).float()[None].to(DEVICE) / 255.0\n",
    "\n",
    "            padder = InputPadder(image1.shape)\n",
    "            image1, image2 = padder.pad(image1, image2)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, flow_up = raft(image1, image2, iters=20, test_mode=True)\n",
    "\n",
    "            flow = flow_up[0].permute(1, 2, 0).cpu().numpy()\n",
    "            flow = cv2.resize(flow, (x2 - x1, y2 - y1), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            step = 10\n",
    "            h_crop, w_crop = flow.shape[:2]\n",
    "            yy, xx = np.mgrid[step//2:h_crop:step, step//2:w_crop:step]\n",
    "            fx = flow[yy, xx, 0]\n",
    "            fy = flow[yy, xx, 1]\n",
    "\n",
    "            flow_magnitude = np.sqrt(fx**2 + fy**2)\n",
    "            flow_magnitude_filtered = scipy.ndimage.median_filter(flow_magnitude, size=3)\n",
    "\n",
    "            mean_pixel_movement = np.mean(flow_magnitude_filtered)\n",
    "            mean_meter_movement = mean_pixel_movement * meters_per_pixel\n",
    "            time_seconds = interval / fps\n",
    "            smoke_speed_mps = mean_meter_movement / time_seconds\n",
    "\n",
    "            label = f\"Speed: {smoke_speed_mps:.2f} m/s\"\n",
    "            cv2.rectangle(frame2, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            cv2.putText(frame2, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            for i in range(len(xx.flat)):\n",
    "                pt1 = (x1 + int(xx.flat[i]), y1 + int(yy.flat[i]))\n",
    "                pt2 = (x1 + int(xx.flat[i] + fx.flat[i]), y1 + int(yy.flat[i] + fy.flat[i]))\n",
    "                cv2.arrowedLine(frame2, pt1, pt2, (0, 255, 0), 1, tipLength=0.3)\n",
    "\n",
    "        out.write(frame2)\n",
    "        cv2.imshow(\"RAFT Flow Overlay\", frame2)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    prev_frame = frame.copy()\n",
    "\n",
    "\n",
    "# --------------------------- Cleanup --------------------------- #\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "335d2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    # box = [x1, y1, x2, y2]\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    if union_area == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return inter_area / union_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40ca32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_boxes(boxes, iou_threshold=0.3):\n",
    "    merged = []\n",
    "    used = [False] * len(boxes)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        # Start a new cluster\n",
    "        current = boxes[i]\n",
    "        for j in range(i + 1, len(boxes)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            if compute_iou(current, boxes[j]) > iou_threshold:\n",
    "                # Merge boxes: union of areas\n",
    "                x1 = min(current[0], boxes[j][0])\n",
    "                y1 = min(current[1], boxes[j][1])\n",
    "                x2 = max(current[2], boxes[j][2])\n",
    "                y2 = max(current[3], boxes[j][3])\n",
    "                current = [x1, y1, x2, y2]\n",
    "                used[j] = True\n",
    "        merged.append(current)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcb42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'Axis-Alpine', 'name': '', 'last_frame_ts': 1745892544, 'fov_lft': [None, None], 'fov_rt': [None, None], 'fov_center': [None, None], 'fov': '', 'ProdNbr': None, 'az_current': 151.42999267578125, 'tilt_current': 0.0, 'zoom_current': 1.0, 'is_patrol_mode': 0, 'is_currently_patrolling': 0, 'state': '', 'county': '', 'isp': '', 'sponsor': '', 'region': ''}\n"
     ]
    }
   ],
   "source": [
    "print(camera_info['properties'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34606bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_coord(coord):\n",
    "    if coord is None or len(coord) != 2:\n",
    "        return False\n",
    "    lat, lon = coord\n",
    "    if lat is None or lon is None:\n",
    "        return False\n",
    "    if not (-90 <= lat <= 90):\n",
    "        return False\n",
    "    if not (-180 <= lon <= 180):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7961d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 cameras with GOOD FOV!\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "good_cameras = []\n",
    "for feature in camera_data['features']:\n",
    "    props = feature['properties']\n",
    "    left = props.get('fov_lft')\n",
    "    right = props.get('fov_rt')\n",
    "\n",
    "    if left and right and is_valid_coord(left) and is_valid_coord(right):\n",
    "        good_cameras.append(props['id'])\n",
    "\n",
    "print(f\"Found {len(good_cameras)} cameras with GOOD FOV!\")\n",
    "print(good_cameras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d30738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: AXIS-BCPOSLongmont\n",
      "name: \n",
      "last_frame_ts: 1745892834\n",
      "fov_lft: [None, None]\n",
      "fov_rt: [None, None]\n",
      "fov_center: [None, None]\n",
      "fov: \n",
      "ProdNbr: None\n",
      "az_current: 270.0\n",
      "tilt_current: -0.6800000071525574\n",
      "zoom_current: 1.0\n",
      "is_patrol_mode: 0\n",
      "is_currently_patrolling: 0\n",
      "state: \n",
      "county: \n",
      "isp: \n",
      "sponsor: \n",
      "region: \n"
     ]
    }
   ],
   "source": [
    "# Fetch latest cameras\n",
    "now = datetime.now()\n",
    "camera_data = get_camera_json(now)\n",
    "\n",
    "# Pick first camera just as example\n",
    "sample_camera = camera_data['features'][0]['properties']\n",
    "\n",
    "# Print all keys and their values\n",
    "for key, value in sample_camera.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d75b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
