{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR Evaluation on Roboflow Wildfire Smoke Test Set (Local)\n\nThis notebook loads a DETR model checkpoint (trained on Roboflow YOLO-format wildfire smoke dataset) and evaluates it on the test split, reporting mAP and visualizing predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Uncomment if running for the first time\n# !pip install transformers torchvision torchmetrics pycocotools ruamel.yaml opencv-python tqdm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nimport torch\nfrom torchvision.datasets import CocoDetection\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom transformers import DetrForObjectDetection, DetrConfig, DetrImageProcessor\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nfrom ruamel.yaml import YAML\nimport json\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Paths and Load Class Names"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_root = './roboflow'  # Update if needed\nyaml_path = os.path.join(data_root, 'data.yaml')\ncheckpoint_path = './detr_last.ckpt'  # Path to your downloaded checkpoint\n\nyaml = YAML()\nwith open(yaml_path, 'r') as f:\n    data_yaml = yaml.load(f)\nclass_list = data_yaml['names'] if 'names' in data_yaml else data_yaml['nc']\nprint('Class names:', class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert YOLO to COCO for Test Set (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def yolo_to_coco(img_dir, label_dir, class_list, output_json):\n    images = []\n    annotations = []\n    annotation_id = 1\n    img_files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n    for img_id, filename in enumerate(img_files, 1):\n        img_path = os.path.join(img_dir, filename)\n        img = Image.open(img_path)\n        width, height = img.size\n        images.append({\n            'id': img_id,\n            'file_name': filename,\n            'width': width,\n            'height': height\n        })\n        label_path = os.path.join(label_dir, filename.rsplit('.', 1)[0] + '.txt')\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) == 5:\n                        class_id, x_center, y_center, w, h = map(float, parts)\n                        x = (x_center - w/2) * width\n                        y = (y_center - h/2) * height\n                        w_box = w * width\n                        h_box = h * height\n                        annotations.append({\n                            'id': annotation_id,\n                            'image_id': img_id,\n                            'category_id': int(class_id) + 1,\n                            'bbox': [x, y, w_box, h_box],\n                            'area': w_box * h_box,\n                            'iscrowd': 0\n                        })\n                        annotation_id += 1\n    categories = [{\"id\": i+1, \"name\": name} for i, name in enumerate(class_list)]\n    coco_dict = {'images': images, 'annotations': annotations, 'categories': categories}\n    with open(output_json, 'w') as f:\n        json.dump(coco_dict, f)\n    print(f'COCO annotation saved to {output_json}')\n    return coco_dict\n\ntest_img_dir = os.path.join(data_root, 'test', 'images')\ntest_label_dir = os.path.join(data_root, 'test', 'labels')\ntest_coco_json = os.path.join(data_root, 'test_coco.json')\nif not os.path.exists(test_coco_json):\n    yolo_to_coco(test_img_dir, test_label_dir, class_list, test_coco_json)\nelse:\n    print(f'COCO annotation already exists: {test_coco_json}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare DataLoader for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def detr_collate_fn(batch):\n    images, targets = list(zip(*batch))\n    new_targets = []\n    for t in targets:\n        boxes = []\n        labels = []\n        for obj in t:\n            boxes.append(obj['bbox'])\n            labels.append(obj['category_id'] - 1)\n        if boxes:\n            boxes = torch.tensor(boxes, dtype=torch.float32)\n            labels = torch.tensor(labels, dtype=torch.int64)\n        else:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros((0,), dtype=torch.int64)\n        new_targets.append({'boxes': boxes, 'labels': labels})\n    return images, new_targets\n\ntransform = transforms.Compose([transforms.ToTensor()])\ntest_dataset = CocoDetection(test_img_dir, test_coco_json, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=detr_collate_fn)\nprint('Test set size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load DETR Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nconfig = DetrConfig.from_pretrained('facebook/detr-resnet-50')\nconfig.num_labels = len(class_list)\nmodel = DetrForObjectDetection(config)\n# Load checkpoint\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nif 'state_dict' in checkpoint:  # Lightning checkpoint\n    model.load_state_dict({k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items() if k.startswith('model.')})\nelse:\n    model.load_state_dict(checkpoint)\nmodel = model.to(device)\nmodel.eval()\nprocessor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')\nprint('Loaded DETR model from checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set (mAP and Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "map_metric = MeanAveragePrecision(class_metrics=True)\nall_losses = []\nfor images, targets in tqdm(test_loader):\n    with torch.no_grad():\n        encoding = processor(images, return_tensors=\"pt\", do_rescale=False).to(device)\n        outputs = model(**encoding)\n    # Post-process outputs\n    if isinstance(images[0], torch.Tensor):\n        target_sizes = torch.stack([torch.tensor(img.shape[-2:]) for img in images]).to(device)\n    else:\n        target_sizes = torch.tensor([img.size[::-1] for img in images]).to(device)\n    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.5)\n    # Format results and targets for metric\n    for r in results:\n        r[\"boxes\"] = r[\"boxes\"].to(device)\n        r[\"labels\"] = r[\"labels\"].to(device)\n        r[\"scores\"] = r[\"scores\"].to(device)\n    formatted_targets = []\n    for t in targets:\n        formatted_targets.append({\n            \"boxes\": t[\"boxes\"].to(device),\n            \"labels\": t[\"labels\"].to(device)\n        })\n    map_metric.update(results, formatted_targets)\nmetrics = map_metric.compute()\nfor k in list(metrics.keys()):\n    if isinstance(metrics[k], torch.Tensor) and metrics[k].numel() > 1:\n        valid = metrics[k][metrics[k] != -1]\n        metrics[k + '_mean'] = valid.mean().item() if valid.numel() > 0 else float('nan')\n        del metrics[k]\nscalar_metrics = {k: (v.item() if isinstance(v, torch.Tensor) and v.numel() == 1 else v) for k, v in metrics.items() if isinstance(v, (float, int)) or (isinstance(v, torch.Tensor) and v.numel() == 1)}\nprint('Test metrics:', scalar_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_predictions(model, processor, dataloader, device, class_names, num_images=4, score_threshold=0.5):\n    model.eval()\n    images_shown = 0\n    for images, targets in dataloader:\n        pixel_values = processor(images, return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n        with torch.no_grad():\n            outputs = model(pixel_values=pixel_values)\n        results = processor.post_process_object_detection(outputs, target_sizes=[img.shape[1:] for img in images], threshold=score_threshold)\n        for idx, (image, result) in enumerate(zip(images, results)):\n            plt.figure(figsize=(8, 6))\n            img_np = image.permute(1, 2, 0).cpu().numpy()\n            plt.imshow(img_np)\n            ax = plt.gca()\n            boxes = result[\"boxes\"].cpu().numpy()\n            scores = result[\"scores\"].cpu().numpy()\n            labels = result[\"labels\"].cpu().numpy()\n            for box, score, label in zip(boxes, scores, labels):\n                xmin, ymin, xmax, ymax = box\n                ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red', linewidth=2))\n                ax.text(xmin, ymin, f'{class_names[label]}: {score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5), fontsize=10, color='black')\n            plt.axis('off')\n            plt.show()\n            images_shown += 1\n            if images_shown >= num_images:\n                return\n\n# Usage example:\nplot_predictions(model, processor, test_loader, device, class_list, num_images=6, score_threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
