{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR Training on Roboflow YOLO-format Wildfire Smoke Dataset (Colab Ready)\n",
    "\n",
    "This notebook is designed for Google Colab and will train DETR on a YOLO-format wildfire smoke dataset structured as Roboflow exports, with splits for train, valid, and test. It automatically parses the dataset structure and class names from `data.yaml`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\ndrive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q transformers torchvision pytorch-lightning torchmetrics pycocotools ruamel.yaml opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse Dataset Structure and Class Names"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\nfrom ruamel.yaml import YAML\n\ndata_root = '/content/drive/MyDrive/smoke_dataset/roboflow'\nyaml_path = os.path.join(data_root, 'data.yaml')\nyaml = YAML()\nwith open(yaml_path, 'r') as f:\n    data_yaml = yaml.load(f)\n\n# Get class names\nclass_list = data_yaml['names'] if 'names' in data_yaml else data_yaml['nc']\nprint('Class names:', class_list)\n\nsplits = ['train', 'valid', 'test']\nfor split in splits:\n    print(f'{split} images:', os.listdir(os.path.join(data_root, split, 'images'))[:3])\n    print(f'{split} labels:', os.listdir(os.path.join(data_root, split, 'labels'))[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert YOLO to COCO for Each Split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import glob\nimport json\nfrom PIL import Image\nfrom tqdm import tqdm\n\ndef yolo_to_coco(img_dir, label_dir, class_list, output_json):\n    images = []\n    annotations = []\n    annotation_id = 1\n    img_files = sorted(glob.glob(os.path.join(img_dir, '*')))\n    for img_id, img_path in enumerate(tqdm(img_files), 1):\n        img = Image.open(img_path)\n        width, height = img.size\n        filename = os.path.basename(img_path)\n        images.append({\n            'id': img_id,\n            'file_name': filename,\n            'width': width,\n            'height': height\n        })\n        label_path = os.path.join(label_dir, filename.rsplit('.', 1)[0] + '.txt')\n        if os.path.exists(label_path):\n            with open(label_path, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) == 5:\n                        class_id, x_center, y_center, w, h = map(float, parts)\n                        x = (x_center - w/2) * width\n                        y = (y_center - h/2) * height\n                        w_box = w * width\n                        h_box = h * height\n                        annotations.append({\n                            'id': annotation_id,\n                            'image_id': img_id,\n                            'category_id': int(class_id) + 1,\n                            'bbox': [x, y, w_box, h_box],\n                            'area': w_box * h_box,\n                            'iscrowd': 0\n                        })\n                        annotation_id += 1\n    categories = [{\"id\": i+1, \"name\": name} for i, name in enumerate(class_list)]\n    coco_dict = {'images': images, 'annotations': annotations, 'categories': categories}\n    with open(output_json, 'w') as f:\n        json.dump(coco_dict, f)\n    print(f'COCO annotation saved to {output_json}')\n    return coco_dict\n\ncoco_jsons = {}\nfor split in splits:\n    img_dir = os.path.join(data_root, split, 'images')\n    label_dir = os.path.join(data_root, split, 'labels')\n    output_json = os.path.join(data_root, f'{split}_coco.json')\n    coco_jsons[split] = output_json\n    yolo_to_coco(img_dir, label_dir, class_list, output_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare DataLoaders for Each Split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision.datasets import CocoDetection\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\ntransform = transforms.Compose([transforms.ToTensor()])\n\ndatasets = {}\ndataloaders = {}\nfor split in splits:\n    img_dir = os.path.join(data_root, split, 'images')\n    coco_json = coco_jsons[split]\n    datasets[split] = CocoDetection(img_dir, coco_json, transform=transform)\n    dataloaders[split] = DataLoader(datasets[split], batch_size=4, shuffle=(split=='train'), num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n\nprint('Train set size:', len(datasets['train']))\nprint('Valid set size:', len(datasets['valid']))\nprint('Test set size:', len(datasets['test']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define DETR Lightning Module (with metrics logging)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import DetrForObjectDetection, DetrConfig, DetrImageProcessor\nimport torch\nimport pytorch_lightning as pl\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\n\nclass DETRLightningModule(pl.LightningModule):\n    def __init__(self, num_classes):\n        super().__init__()\n        config = DetrConfig.from_pretrained('facebook/detr-resnet-50')\n        config.num_labels = num_classes\n        self.model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50', config=config, ignore_mismatched_sizes=True)\n        self.processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50')\n        self.map_metric = MeanAveragePrecision(class_metrics=True)\n\n    def forward(self, pixel_values):\n        return self.model(pixel_values)\n\n    def training_step(self, batch, batch_idx):\n        images, targets = batch\n        encoding = self.processor(images, return_tensors=\"pt\").to(self.device)\n        labels = [{\"class_labels\": t[\"labels\"], \"boxes\": t[\"boxes\"]} for t in targets]\n        outputs = self.model(**encoding, labels=labels)\n        loss = outputs.loss\n\n        if isinstance(images[0], torch.Tensor):\n            target_sizes = torch.stack([torch.tensor(img.shape[-2:]) for img in images]).to(self.device)\n        else:\n            target_sizes = torch.tensor([img.size[::-1] for img in images]).to(self.device)\n\n        results = self.processor.post_process_object_detection(\n            outputs, target_sizes=target_sizes, threshold=0.5\n        )\n        for r in results:\n            r[\"boxes\"] = r[\"boxes\"].to(self.device)\n            r[\"labels\"] = r[\"labels\"].to(self.device)\n            r[\"scores\"] = r[\"scores\"].to(self.device)\n        formatted_targets = []\n        for t in targets:\n            formatted_targets.append({\n                \"boxes\": t[\"boxes\"].to(self.device),\n                \"labels\": t[\"labels\"].to(self.device)\n            })\n        self.map_metric.update(results, formatted_targets)\n        metrics = self.map_metric.compute()\n        for k in list(metrics.keys()):\n            if isinstance(metrics[k], torch.Tensor) and metrics[k].numel() > 1:\n                valid = metrics[k][metrics[k] != -1]\n                metrics[k + '_mean'] = valid.mean().item() if valid.numel() > 0 else float('nan')\n                del metrics[k]\n        scalar_metrics = {k: (v.item() if isinstance(v, torch.Tensor) and v.numel() == 1 else v) for k, v in metrics.items() if isinstance(v, (float, int)) or (isinstance(v, torch.Tensor) and v.numel() == 1)}\n        self.log_dict(scalar_metrics, prog_bar=True, on_step=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, targets = batch\n        encoding = self.processor(images, return_tensors=\"pt\").to(self.device)\n        labels = [{\"class_labels\": t[\"labels\"], \"boxes\": t[\"boxes\"]} for t in targets]\n        outputs = self.model(**encoding, labels=labels)\n        loss = outputs.loss\n\n        if isinstance(images[0], torch.Tensor):\n            target_sizes = torch.stack([torch.tensor(img.shape[-2:]) for img in images]).to(self.device)\n        else:\n            target_sizes = torch.tensor([img.size[::-1] for img in images]).to(self.device)\n        results = self.processor.post_process_object_detection(\n            outputs, target_sizes=target_sizes, threshold=0.5\n        )\n        for r in results:\n            r[\"boxes\"] = r[\"boxes\"].to(self.device)\n            r[\"labels\"] = r[\"labels\"].to(self.device)\n            r[\"scores\"] = r[\"scores\"].to(self.device)\n        formatted_targets = []\n        for t in targets:\n            formatted_targets.append({\n                \"boxes\": t[\"boxes\"].to(self.device),\n                \"labels\": t[\"labels\"].to(self.device)\n            })\n        self.map_metric.update(results, formatted_targets)\n        metrics = self.map_metric.compute()\n        for k in list(metrics.keys()):\n            if isinstance(metrics[k], torch.Tensor) and metrics[k].numel() > 1:\n                valid = metrics[k][metrics[k] != -1]\n                metrics[k + '_mean'] = valid.mean().item() if valid.numel() > 0 else float('nan')\n                del metrics[k]\n        scalar_metrics = {k: (v.item() if isinstance(v, torch.Tensor) and v.numel() == 1 else v) for k, v in metrics.items() if isinstance(v, (float, int)) or (isinstance(v, torch.Tensor) and v.numel() == 1)}\n        self.log_dict(scalar_metrics, prog_bar=True, on_step=False, on_epoch=True)\n        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = DETRLightningModule(num_classes=len(class_list))\ntrainer = pl.Trainer(max_epochs=10, accelerator='gpu' if torch.cuda.is_available() else 'cpu')\ntrainer.fit(model, dataloaders['train'], dataloaders['valid'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Metrics and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\nimport torch\n\ndef plot_predictions(model, processor, dataloader, device, class_names, num_images=4, score_threshold=0.5):\n    model.eval()\n    images_shown = 0\n    for images, targets in dataloader:\n        pixel_values = processor(images, return_tensors=\"pt\").pixel_values.to(device)\n        with torch.no_grad():\n            outputs = model(pixel_values=pixel_values)\n        results = processor.post_process_object_detection(outputs, target_sizes=[img.shape[1:] for img in images], threshold=score_threshold)\n        for idx, (image, result) in enumerate(zip(images, results)):\n            plt.figure(figsize=(8, 6))\n            img_np = image.permute(1, 2, 0).cpu().numpy()\n            plt.imshow(img_np)\n            ax = plt.gca()\n            boxes = result[\"boxes\"].cpu().numpy()\n            scores = result[\"scores\"].cpu().numpy()\n            labels = result[\"labels\"].cpu().numpy()\n            for box, score, label in zip(boxes, scores, labels):\n                xmin, ymin, xmax, ymax = box\n                ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color='red', linewidth=2))\n                ax.text(xmin, ymin, f'{class_names[label]}: {score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5), fontsize=10, color='black')\n            plt.axis('off')\n            plt.show()\n            images_shown += 1\n            if images_shown >= num_images:\n                return\n# Usage example:\n# plot_predictions(model.model, model.processor, dataloaders['test'], device=\"cuda\" if torch.cuda.is_available() else \"cpu\", class_names=class_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
